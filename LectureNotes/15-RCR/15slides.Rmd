---
title: "Lecture 15"
subtitle: "Causality without a valid identification strategy"
author: Tyler Ransom
date: ECON 6343, University of Oklahoma
output:
  xaringan::moon_reader:
    includes:
        in_header: "15slides_files/mathjax-equation-numbers.html"
    css: ['default', 'metropolis', 'metropolis-fonts', 'ou-colors.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
      beforeInit: ["https://platform.twitter.com/widgets.js"]
    seal: false

---

```{r, load_refs, include=FALSE, cache=FALSE}
library(RefManageR)
library(tidyverse)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           style = "markdown",
           hyperlink = FALSE,
           dashed = TRUE)
biblio <- ReadBib("../../References/References.bib", check = FALSE)
```

class: title-slide

<br><br><br>

# Lecture 15

## Causality without a valid identification strategy

### Tyler Ransom

### ECON 6343, University of Oklahoma

---

# Plan for the Day

1. The current state of linear IV

2. How can we infer causality when no valid "strategy" exists?

3. Examples of papers pursuing this approach

4. Application: _SFFA v. Harvard_ lawsuit


---
# The current state of linear IV

- Over the past two years, instrumental variables have taken a beating

- `r Citet(biblio,"young2020")` looks at papers published in AEA journals: 

    - Published IV results are highly sensitive (i.e. depend on small no. of obs.)
    
    - IV has little power (i.e. rarely rejects OLS point estimate)
    
    - Statistical significance of excluded instruments is exaggerated

- `r Citet(biblio,"lee_al2020")`: valid instrument requires $F>105$

- `r Citet(biblio,"mellon2020")`: so many papers use rainfall as IV, it must not be excludable

- On the bright side, #EconTwitter has created some good niche content


---
# Rainfall as an instrument

.center[
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">(Gleefully rubs hands together while searching twitter for “<a href="https://twitter.com/alex_peys?ref_src=twsrc%5Etfw">@Alex_peys</a> rainfall meme”) <a href="https://t.co/HV0DNKOUKw">pic.twitter.com/HV0DNKOUKw</a></p>&mdash; ⑆Luke Stein⑈ (@lukestein) <a href="https://twitter.com/lukestein/status/1298665911101136901?ref_src=twsrc%5Etfw">August 26, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
]

---
# Is the instrument excludable?
.center[
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Nice work from <a href="https://twitter.com/jon_mellon?ref_src=twsrc%5Etfw">@jon_mellon</a>. So many papers use weather as an IV for different measures that this strongly suggests exclusion-restriction violations are everywhere. Read it also for his absolutely relentless commitment to the bit in the section headings. <a href="https://t.co/Ih3si6CIgX">https://t.co/Ih3si6CIgX</a> <a href="https://t.co/2QDIQIoaNy">pic.twitter.com/2QDIQIoaNy</a></p>&mdash; Kieran Healy (@kjhealy) <a href="https://twitter.com/kjhealy/status/1318593081894182913?ref_src=twsrc%5Etfw">October 20, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
]

---
# Beyond IV

- It has become more clear that IV has significant limitations

- How can we still infer causality from observational cross-sectional data?

- We know that whatever instrument we might imagine up is unlikely to be valid

- We have three remaining options:

    - Try to include enough controls so as to satisfy unconfoundedness
    
    - Walk away and concede that we'll never be able to obtain a causal estimate
    
    - Partially identify the effect of interest

---
# Setting

- Suppose we have the following model we'd like to estimate

\begin{align*}
y &= \alpha d + X\beta + \varepsilon
\end{align*}

- We want to estimate $\alpha$ such that we can infer causality of treatment $d$ on $y$

- But since we only have observational data, this is a tall task

- Today we'll focus on partial identification

- .hi[Partial identification] means we identify a _set_ of values $\alpha$ can take on

- One edge of the set is the "assume correlation is causality" bound

- The other edge is a plausible lower bound (if bias on $\alpha$ is positive)


---
# Intuition

- Thinking again about our model (and assume $X$ includes an intercept),

\begin{align*}
y &= \alpha d + X\beta + \varepsilon
\end{align*}

suppose $Corr(X\beta,\varepsilon)=0$ but $Corr(d,\varepsilon)\neq0$

- Can we (causally) infer something about $\alpha$ from $Corr(d,X\beta)$?

- Specifically, how would $\hat{\alpha}$ change if $Corr(d,\varepsilon)=Corr(d,X\beta)$? Or if $Corr(d,\varepsilon)=0$?

- Knowing $d$'s correlation w/unobservables $(\varepsilon)$ can help us pin down causality

- We can use the correlation of $d$ with the observables $(X\beta)$ as a guide


---
# More intuition

- The following steps allow us to identify the causal effect

    1. Regress $y$ on $d$
    
    2. Regress $y$ on $d$ and $X$
    
    3. Compute the magnitude of the change in $\alpha$ across (1) and (2)
    
    4. Make an assumption about how $Corr(d,\varepsilon)$ and $Corr(d,X\beta)$ relate
    
    5. This allows you to plausibly compute the unbiased (i.e. causal) value of $\alpha$

---
# `r Citet(biblio,"altonji_al2005jpe")`

- The authors pioneer the idea of bounding $Corr(d,\varepsilon)$ by looking at $Corr(d,X\beta)$

- Setting: estimating the causal effect of attending Catholic high school

- There is lots of selection into this process, but no available random variation

- Argue that $Corr(d,\varepsilon)=Corr(d,X\beta)$ is a good bound to the set. Why?

    - $X$ is only a subset of everything that affects $y$
    
    - This is because data is often collected for multiple purposes
    
    - Data is also costly to collect, and some variables are impossible to measure
    
    - Thus $X$ is probably a _random_ subset of everything that affects $y$

---
# $Corr(d,\varepsilon)\overset{?}{\lesseqqgtr}Corr(d,X\beta)$

- How do we know if we should assume $Corr(d,\varepsilon)=Corr(d,X\beta)$?

- This question is application-specific and requires careful thinking

- What are the sources of selection bias? Why is $R^2$ low?

- Selection bias? Measurement error? Irreducible uncertainty?

- e.g. if there is a lot of irreducible uncertainty, then  $Corr(d,\varepsilon)<Corr(d,X\beta)$

- The opposite is true if there is not much irreducible uncertainty

- Also, typically assume that $Corr(d,\varepsilon)$ and $Corr(d,X\beta)$ have same sign


---
# `r Citet(biblio,"krauth2016")`

- Generalizes `r Citet(biblio,"altonji_al2005jpe")`

- Allows for a .hi[relative correlation restriction (RCR)]

\begin{align*}
Corr(d, \varepsilon) = \lambda Corr(d, X\beta)
\end{align*}

- We can do two things with $\lambda$:

    1. Assume $\lambda \in [\lambda_L, \lambda_H]$ and then estimate corresponding $\alpha$'s in the interval $[\alpha_L, \alpha_H]$
    
    2. Estimate $\alpha$ by OLS, then find the smallest (absolute) value of $\lambda$ such that the OLS estimate is statistically zero


---
# `r Citet(biblio,"oster2019")`

- Also generalizes `r Citet(biblio,"altonji_al2005jpe")`

- Focuses on comparing movements in $\alpha$ with corresponding movements in $R^2$

- Intuition: if we could observe all unobservables, then $R^2=1$

- Thus, the value of $\alpha$ when $R^2=1$ represents its true causal value

    - If there is measurement error in $y$, instead consider $R_{\max}<1$

- Implementation-wise, the approach is closely similar to `r Citet(biblio,"krauth2016")`

- I prefer Krauth's approach, but the idea is the same


---
# `r Citet(biblio,"diegert_al2022")`

- The three prior papers each assume that $Corr(X, \varepsilon) = 0$

- i.e. the $X$'s (control variables) are themselves exogenous

- But this is rarely true in practice

- Propose thinking about an extended model:

\begin{align*}
y &= \alpha d + X\beta + \underbrace{W\gamma + \nu}_{ = \varepsilon}
\end{align*}
where $W$ represents unobservable variables and $\nu$ is a pure error term

- Other methods allow $d$ and $W$ to be correlated through $Corr(d, \varepsilon)$ 

- but do not allow $X$ and $W$ to be correlated

---
# Terminology of `r Citet(biblio,"diegert_al2022")`

- .hi[long regression:] regress $Y$ on $d$, $X$ and $W$

    - this is the regression we want to run but can't because $W$ is unobserved

- .hi[medium regression:] regress $Y$ on $d$ and $X$

    - this is the regression we can run, but which may be of limited use

- $\alpha_{\text{long}}$ is the true treatment effect, but $\alpha_{\text{med}}$ is the one we can feasibly estimate

- .hi[breakdown point:] largest magnitude of selection on unobservables relative to observables needed to overturn a specific baseline finding

- the breakdown point is a single number that summarizes a study's robustness to selection on unobservables while also allow for the $X$'s to be endogenous

---
# Sensitivity parameters

- There are three sensitivity parameters $(\overline{r}_X, \overline{r}_Y, \overline{c})$ which are defined as follows:

$\overline{r}_X$ measures how correlated treatment is to the index of unobservables compared to the index of observables

\begin{align*}
Corr(d, W\gamma) &\leq \overline{r}_X Corr(d, X\beta)
\end{align*}

$\overline{r}_y$ measures how correlated the outcome is to the index of unobservables compared to the index of observables

\begin{align*}
Corr(y, W\gamma) &\leq \overline{r}_Y Corr(y, X\beta)
\end{align*}

$\overline{c}$ measures the partial $R^2$ of $W$ above and beyond $X$ in the long regression
\begin{align*}
\text{partial  }R^2_{W,X} &\leq \overline{c}
\end{align*}

---
# Breakdown point

- The .hi[breakdown point] $\overline{r}_{X}^{\text{bp}}$ is the largest value of $\overline{r}_X$ such that $\alpha_{\text{long}}$ would be 0

- The formula for $\overline{r}_{X}^{\text{bp}}$ is a function of $R^2_{Y\sim d \cdot X}$ and partial $R^2$ terms:

\begin{align*}
\overline{r}_{X}^{\text{bp}} & = \left( \frac{R^2_{Y\sim d \cdot X}}{ \frac{R^2_{d\sim X}}{1 - R^2_{d\sim X}} + R^2_{Y\sim d \cdot X}  } \right)^{\frac{1}{2}}
\end{align*}

where $R^2_{Y\sim d \cdot X}$ means "the $R^2$ from a regression of $y$ on $d$ and $X$"

In other words,
\begin{align*}
\text{Selection on Unobservables} & = r \cdot (\text{Selection on Observables})
\end{align*}
and $\overline{r}_{X}^{\text{bp}}$ is a bound on $r$

---
# Coding up these estimators

- `r Citet(biblio,"altonji_al2005jpe")` don't provide generalizable code for their approach

- `r Citet(biblio,"krauth2016")`, `r Citet(biblio,"oster2019")`, and `r Citet(biblio,"diegert_al2022")` each separately have Stata packages

- `r Citet(biblio,"diegert_al2022")` may also have a Python package

- There's nothing in Julia or R that I am aware of, but I know that `r Citet(biblio,"diegert_al2022")` are planning to develop an R package

- A somewhat related approach by `r Citet(biblio,"cinelliHazlett2020")` has an accompanying R package called [sensemakr](https://github.com/carloscinelli/sensemakr)

---
# Example Stata code

- Below is a code snippet that shows how to use each of these Stata packages

- The [full do-file](https://github.com/OU-PhD-Econometrics/fall-2022/blob/master/LectureNotes/15-RCR/sensitivity_examples.do) is available on GitHub

.scroll-box-12[
```{c++,eval=F}
// load the data
use bfg2020, clear

// create dummies
qui tab statea, gen(statea_d)

// define local macros
local y avgrep2000to2016
local d tye_tfe890_500kNI_100_l6
local x1 log_area_2010 lat lon temp_mean rain_mean elev_mean d_coa d_riv d_lak ave_gyi
local x0 statea_d*
local x `x1' `x0'
local SE cluster(km_grid_cel_code)

// OLS: "short" model
reg `y' `d', `SE'

// OLS: "medium" model
reg `y' `d' `x', `SE'

// Krauth
rcr `y' `d' `x', lambda(0 0.1) cluster(km_grid_cel_code)

// Oster
qui reg `y' `d' `x', `SE'
psacalc beta `d', delta(0) // returns "medium" OLS result
psacalc beta `d'           // returns an estimate of coeff. of interest ("beta") when ratio of sel. on obs. ÷ sel. on unobs. ("delta") is equal to 1
psacalc delta `d', beta(0) rmax(0.7) // reports ratio of sel. on obs. ÷ sel. on unobs. ("delta") such that coeff. of interest ("beta") is equal to 0

// Diegert et al.
regsensitivity `y' `d' `x', compare(`x1') //  Breakdown point = 80.4%, meaning that the OLS coefficient becomes 0 if treatment is 80% as correlated with unobservables as it is with observables
regsensitivity bounds `y' `d' `x', compare(`x1') // only reports the bounds
```
]


---
# Papers that use these approaches

- `r Citet(biblio,"altonji_al2005jpe")` analyze Catholic school value-added

    - Catholic HS increases HS graduation, but not test scores or college attendance

- `r Citet(biblio,"ransomRansom2018")` analyze returns to being a HS athlete

    - Virtually all of the effect is selection
    
    - Use Krauth's method
    
    - $\lambda^\ast$ (breakdown point) typically very close to 0
    
    - This implies that $\alpha\rightarrow 0$ for even slight deviations of $Corr(d,\varepsilon)$ from 0
    
    - Thus, it's unlikely that being a HS athlete causes much of any later outcome

---
# When it's impossible to randomize

- There are many situations where it would be impossible to run an experiment

- Some examples:

    - Almost all job hiring
    
    - College or graduate school admissions
    
    - Many others
    
- In these situations, how can we infer causality?

- If we want to test for racial/sex/age discrimination, how can we do it?

- The methods discussed today can be useful in testing for discrimination

---
# Case study: _SFFA v. Harvard_

- In 2014, Students For Fair Admissions brought a legal complaint against Harvard

- SFFA claimed that Harvard discriminated against Asian Americans in admissions

- In October 2018, the case went to trial

- Expert witnesses: Peter Arcidiacono (SFFA), David Card (Harvard)

- Expert witness reports were publicly released in June 2018 ([1](https://docs.justia.com/cases/federal/district-courts/massachusetts/madce/1:2014cv14176/165519/415/1.html), [2](https://projects.iq.harvard.edu/files/diverse-education/files/expert_report_-_2017-12-15_dr._david_card_expert_report_updated_confid_desigs_redacted.pdf), [3](https://docs.justia.com/cases/federal/district-courts/massachusetts/madce/1:2014cv14176/165519/415/2.html), [4](https://projects.iq.harvard.edu/files/diverse-education/files/expert_report_rebuttal_as_filed_d._mass._14-cv-14176_dckt_000419_037_filed_2018-06-15.pdf))

- The reports included a lot of detail about Harvard's admissions process

- Also included a lot of estimates of various models of Harvard admissions

---
# Harvard's admissions process

- What factors does Harvard consider for undergraduate admissions?

- Basically, everything it can possibly observe:

    - HS transcript data, standardized test scores
    
    - Letters of recommendation, personal statement
    
    - Demographics & family background (race/ethnicity, sex, parent SES)
    
    - Relationship with Harvard (prospective athlete, legacy, donor, etc.)
    
- Harvard distills all of the information above into 5 ratings

    - Academic, Extracurricular, Personal, Athletic, Overall

---
# Comparison of the expert models

- Both experts used similar datasets and similar empirical methods

- Analysis boils down to a binomial logit (admit/not) with lots of $X$'s

- Main disagreement is about what to include in $X$, what subsample to use

- SFFA throws out applicants with special relationships, excludes the personal rating

    - The personal rating seeems to be one place where discrimination happens
    
    - If testing for presence of discrimination, shouldn't put this in $X$
    
- Harvard removes some interactions from $X$, adds some poorly measured variables

---
# Coefficient stability across models (SFFA report)

```{css, echo=F}
    .remark-slide thead, .remark-slide tr:nth-child(2n) {
        background-color: white;
    }
```

Logit Coefficient        | (1)       |   (2) |   (3) |   (4) |   (5)  |
-------------------|-----------|-----------|-----------|-----------|-----------|
African American          | 0.531    | 2.417   | 2.671   | 2.851   | 3.772   
                          |  (0.040) | (0.050) | (0.074) | (0.078) | (0.105) 
Hispanic                  |  0.425   | 1.273   | 1.286   | 1.339   | 1.959   
                          |  (0.039) | (0.044) | (0.063) | (0.067) | (0.085) 
Asian American            | 0.057    | -0.434  | -0.565  | -0.378  | -0.466  
                          |  (0.032) | (0.035) | (0.052) | (0.055) | (0.070) 
Missing                   |  0.012   | -0.283  | -0.348  | -0.330  | -0.379  
                          |  (0.054) | (0.057) | (0.093) | (0.099) | (0.122) 
N | 142,728 | 142,700 | 142,700 | 136,061 | 128,422 
Pseudo R Sq. | 0.078 | 0.260 | 0.262 | 0.283 | __0.556__
Demographics | Y  | Y | Y | Y | Y  
Academics | N  | Y | Y | Y | Y  
Race and Gender Interactions | N  | N | Y | Y | Y  
HS and NBHD Variables | N  | N | N | Y | Y  
Ratings (excluding Personal) | N  | N | N | N | Y 

A pseudo $R^2$ of 0.56 in this context is about the same as a linear $R^2$ of $\approx$ 0.8-0.9

---
# Relative correlations in discrimination studies

- In discrimination studies, $d$ corresponds to the race/sex/age group of interest

- We can't assume $Corr(d,\varepsilon)=0$ since we're using observational data 

- So we want to use $Corr(d,X\beta)$ as a guide

- If $|\alpha|\downarrow$ as more controls are added, argument for discrimination weakens

- The logic being that, if we could add all the controls, $\alpha\rightarrow 0$


---
# Does Harvard discriminate against Asians?

- SFFA put forth the following compelling evidence:

- The coefficient on Asian American is negative & significant, with a high $R^2$

- Given AME (-1pp) and overall admissions rate (5%), this implies a 20% penalty

- In model 5, $Corr(\text{Asian},X\beta)>Corr(\text{White},X\beta)$, but $\alpha_{\text{Asian}}<\alpha_{\text{White}}$!

- In order for $\alpha\rightarrow 0$, Asians would have to be much, much worse than whites on the remaining unobservables

- This is unlikely, given that $Corr(\text{Asian},X\beta)>Corr(\text{White},X\beta)$ with high $R^2$

---
# How did Harvard's expert estimate a null result?

- Need to do _all_ of the following to obtain an insignificant $\hat{\alpha}_{\text{Asian}}$:

    - include the personal rating in $X$
    
    - include parental occupation (which is riddled with measurement error)
    
    - exclude from $X$ interactions between low-SES status and race
    
- See `r Citet(biblio,"akr2022discrim")` for more details

---
# What did the court decide?

- The trial judge ruled in favor of Harvard

- Main arguments defending this decision:

    - Statistical evidence (however strong) is never enough to rule against someone

    - Harvard admissions officers didn't admit to penalizing Asians
    
    - The models by Harvard's expert had lower residual variance

“Finally, SFFA did not present a single Asian American applicant who was overtly discriminated against or who was better qualified than an admitted white applicant when considering the full range of factors that Harvard values in the admissions process.”


---
# Other fun facts about the trial

- Data included one admissions cycle after the complaint was filed

- In this admissions cycle, the Asian American penalty is much smaller

- During the trial, Harvard re-worded its internal policies about personal rating

- Harvard's internal research office ran models that showed an Asian penalty

- Admissions models show a penalty against those who don't report their race

    - This suggests that applicants know they might be discriminated against
    
- Despite admissions being zero-sum, all of Harvard's witnesses testified that "race is only used as a plus factor"

---
# References
.tiny[
```{r refs, echo=FALSE, results="asis"}
PrintBibliography(biblio)
```
]
